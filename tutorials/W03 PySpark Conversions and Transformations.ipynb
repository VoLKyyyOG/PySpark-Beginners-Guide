{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f37e5",
   "metadata": {},
   "source": [
    "# Apache Spark 3.2 (PySpark) Tutorial\n",
    "- Author: Akira Takihara Wang (https://github.com/akiratwang)\n",
    "\n",
    "Tutorial Operating System(s):\n",
    "- Windows 10 and WSL2\n",
    "- Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb868a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:23:50.032174Z",
     "start_time": "2021-11-24T05:23:47.024646Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 16:23:47 WARN Utils: Your hostname, NeonEx resolves to a loopback address: 127.0.1.1; using 10.1.1.247 instead (on interface eth0)\n",
      "21/11/24 16:23:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/24 16:23:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Conversion and Transformations\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee7221bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:23:50.047599Z",
     "start_time": "2021-11-24T05:23:50.044404Z"
    }
   },
   "source": [
    "schema = \"\"\"\n",
    "`VendorID` INT,  \n",
    "`tpep_pickup_datetime` STRING, \n",
    "`tpep_dropoff_datetime` STRING,\n",
    "`passenger_count` INT, \n",
    "`trip_distance` DOUBLE, \n",
    "`pickup_longitude` DOUBLE, \n",
    "`pickup_latitude` DOUBLE,\n",
    "`RateCodeID` INT, \n",
    "`RatecodeID` INT,\n",
    "`store_and_fwd_flag` STRING, \n",
    "`dropoff_longitude` DOUBLE, \n",
    "`dropoff_latitude` DOUBLE,\n",
    "`payment_type` INT, \n",
    "`fare_amount` DOUBLE, \n",
    "`extra` DOUBLE, \n",
    "`mta_tax` DOUBLE, \n",
    "`tip_amount` DOUBLE,\n",
    "`tolls_amount` DOUBLE, \n",
    "`improvement_surcharge` DOUBLE, \n",
    "`total_amount` DOUBLE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44faedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:23:59.541557Z",
     "start_time": "2021-11-24T05:23:59.539281Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "`VendorID` INT,  \n",
    "`tpep_pickup_datetime` STRING, \n",
    "`tpep_dropoff_datetime` STRING,\n",
    "`passenger_count` INT, \n",
    "`trip_distance` DOUBLE, \n",
    "`pickup_longitude` DOUBLE, \n",
    "`pickup_latitude` DOUBLE,\n",
    "`RateCodeID` INT, \n",
    "`store_and_fwd_flag` STRING, \n",
    "`dropoff_longitude` DOUBLE, \n",
    "`dropoff_latitude` DOUBLE,\n",
    "`payment_type` INT, \n",
    "`fare_amount` DOUBLE, \n",
    "`extra` DOUBLE, \n",
    "`mta_tax` DOUBLE, \n",
    "`tip_amount` DOUBLE,\n",
    "`tolls_amount` DOUBLE, \n",
    "`improvement_surcharge` DOUBLE, \n",
    "`total_amount` DOUBLE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87080802",
   "metadata": {},
   "source": [
    "# Transformations and Lazy Evaluation\n",
    "Transformations in PySpark will transform a Spark DataFrame into a _new_ DataFrame _without_ altering the original data. This means that Spark is **immutable** (i.e there is no `inplace=True` argument like some `pandas` methods).\n",
    "\n",
    "For example, operations will return transformed results rather than mutating the original. Therefore, it is quite common to see:\n",
    "- `sdf = sdf.with_some_transformation()`\n",
    "\n",
    "Finally, Spark operations are evaluated lazily. This is because there is a driver under-the-hood which looks to optimize and make your operations more efficient. This means that your data _does not \"move\"_ until called upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98522fe",
   "metadata": {},
   "source": [
    "## Renaming Columns\n",
    "If we work with the full 2015 dataset, you'll notice some inconsistencies in the column names (`RateCodeID` vs `RatecodeID`). To rename fields, we need to use the `.withColumnRenamed(original name, new name)` method."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab9051da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:17:28.234017Z",
     "start_time": "2021-11-24T05:17:28.144240Z"
    }
   },
   "source": [
    "sdf = spark.read.csv('../data/large', schema=schema, header=True) \\\n",
    "    .withColumnRenamed(\"RatecodeID\",\"RateCodeID\") # rename the wrong column into the correct one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3be153c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:02.063786Z",
     "start_time": "2021-11-24T05:24:00.999092Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.csv('../data/sample.csv', schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d3fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:02.124891Z",
     "start_time": "2021-11-24T05:24:02.124882Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1fd82",
   "metadata": {},
   "source": [
    "# Data Type Conversions\n",
    "If we look at our DataFrame, our `datetime` field is of form `1/12/15 0:00` which follows neither formats. Let's resolve this now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8b1a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:02.601111Z",
     "start_time": "2021-11-24T05:24:02.595980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140697d8",
   "metadata": {},
   "source": [
    "There are several ways to do it, but we will go through the most simple approach:\n",
    "- `.withColumn(new column name, expression for the new column)`\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html#pyspark.sql.DataFrame.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a2eb7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:05.591593Z",
     "start_time": "2021-11-24T05:24:05.586229Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "dtime_format = 'd/M/yy H:mm' \n",
    "dtime_cols = ('tpep_pickup_datetime', 'tpep_dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e7e3f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:05.857105Z",
     "start_time": "2021-11-24T05:24:05.761959Z"
    }
   },
   "outputs": [],
   "source": [
    "for dtime_col in dtime_cols:\n",
    "    sdf = sdf.withColumn(dtime_col, \n",
    "                         to_timestamp(sdf['tpep_pickup_datetime'], dtime_format)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5569c16b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:24:06.488126Z",
     "start_time": "2021-11-24T05:24:06.484467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a699d",
   "metadata": {},
   "source": [
    "Now, we've converted it into a `TimestampType` as required. \n",
    "\n",
    "Let's try some more advanced conversions. For example, if we look at the `store_and_fwd_flag`, it actually represents a boolean condition. According to the Data Dictionary though, we currently have `N` and `Y` representing No and Yes respectively. \n",
    "\n",
    "In `pandas`, we would have done something like this:\n",
    "```python\n",
    "df['store_and_fwd_flag_bool'] = (df['store_and_fwd_flag'] == 'Y').astype(bool)\n",
    "```\n",
    "\n",
    "In Spark, we can use the `.cast()` method to cast a column into a specific data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a92a15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T05:25:59.167447Z",
     "start_time": "2021-11-24T05:25:58.916266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>pickup_longitude</th><th>pickup_latitude</th><th>RateCodeID</th><th>store_and_fwd_flag</th><th>dropoff_longitude</th><th>dropoff_latitude</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th></tr>\n",
       "<tr><td>2</td><td>2015-12-01 00:00:00</td><td>2015-12-01 00:00:00</td><td>5</td><td>0.96</td><td>-73.97994232</td><td>40.76538086</td><td>1</td><td>false</td><td>-73.96630859</td><td>40.76308823</td><td>1</td><td>5.5</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.3</td><td>7.8</td></tr>\n",
       "<tr><td>2</td><td>2015-12-01 00:00:00</td><td>2015-12-01 00:00:00</td><td>2</td><td>2.69</td><td>-73.97233582</td><td>40.76237869</td><td>1</td><td>false</td><td>-73.99362946</td><td>40.74599838</td><td>1</td><td>21.5</td><td>0.0</td><td>0.5</td><td>3.34</td><td>0.0</td><td>0.3</td><td>25.64</td></tr>\n",
       "<tr><td>2</td><td>2015-12-01 00:00:00</td><td>2015-12-01 00:00:00</td><td>1</td><td>2.62</td><td>-73.96884918</td><td>40.76453018</td><td>1</td><td>false</td><td>-73.97454834</td><td>40.79164124</td><td>1</td><td>17.0</td><td>0.0</td><td>0.5</td><td>3.56</td><td>0.0</td><td>0.3</td><td>21.36</td></tr>\n",
       "<tr><td>1</td><td>2015-12-01 00:00:00</td><td>2015-12-01 00:00:00</td><td>1</td><td>1.2</td><td>-73.99393463</td><td>40.74168396</td><td>1</td><td>false</td><td>-73.99766541</td><td>40.74746704</td><td>1</td><td>6.5</td><td>0.5</td><td>0.5</td><td>0.2</td><td>0.0</td><td>0.3</td><td>8.0</td></tr>\n",
       "<tr><td>1</td><td>2015-12-01 00:00:00</td><td>2015-12-01 00:00:00</td><td>2</td><td>3.0</td><td>-73.98892212</td><td>40.72698975</td><td>1</td><td>false</td><td>-73.97559357</td><td>40.6968689</td><td>2</td><td>11.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>12.3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|RateCodeID|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
       "|       2| 2015-12-01 00:00:00|  2015-12-01 00:00:00|              5|         0.96|    -73.97994232|    40.76538086|         1|             false|     -73.96630859|     40.76308823|           1|        5.5|  0.5|    0.5|       1.0|         0.0|                  0.3|         7.8|\n",
       "|       2| 2015-12-01 00:00:00|  2015-12-01 00:00:00|              2|         2.69|    -73.97233582|    40.76237869|         1|             false|     -73.99362946|     40.74599838|           1|       21.5|  0.0|    0.5|      3.34|         0.0|                  0.3|       25.64|\n",
       "|       2| 2015-12-01 00:00:00|  2015-12-01 00:00:00|              1|         2.62|    -73.96884918|    40.76453018|         1|             false|     -73.97454834|     40.79164124|           1|       17.0|  0.0|    0.5|      3.56|         0.0|                  0.3|       21.36|\n",
       "|       1| 2015-12-01 00:00:00|  2015-12-01 00:00:00|              1|          1.2|    -73.99393463|    40.74168396|         1|             false|     -73.99766541|     40.74746704|           1|        6.5|  0.5|    0.5|       0.2|         0.0|                  0.3|         8.0|\n",
       "|       1| 2015-12-01 00:00:00|  2015-12-01 00:00:00|              2|          3.0|    -73.98892212|    40.72698975|         1|             false|     -73.97559357|      40.6968689|           2|       11.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = sdf.withColumn('store_and_fwd_flag',\n",
    "                    (sdf[\"store_and_fwd_flag\"] == 'Y').cast('BOOLEAN')\n",
    "      )\n",
    "sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f270f",
   "metadata": {},
   "source": [
    "# TODO TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd056f",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDF) and Pandas UDFs\n",
    "So far, all the functions and methods have been about simple aggregations or filtering rows. However, preprocessing and data cleansing usually requires more powerful tools such as `regex`.\n",
    "\n",
    "Unlike Pandas's `apply()` method (and also `rdd.map()`), we need to do a \"bit\" more work to generate UDFs.\n",
    "\n",
    "1. Create a function with a `@udf()` decorator.\n",
    "2. Specify an output data type (i.e `StringType()`) as format `@udf(\"string\")` or `@udf(StringType())`.\n",
    "3. Apply onto column(s) of choice (remembering that Spark is immutable).\n",
    "\n",
    "Alternatively, if we want to use Pandas framework:\n",
    "1. Create a function with a `@pandas_udf()` decorator and format as required.\n",
    "2. Apply onto column(s) of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfe392",
   "metadata": {},
   "source": [
    "In the following example, we will create a tuple consisting of pickup lat/lon to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808d483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:17.893091Z",
     "start_time": "2021-07-04T02:46:17.890608Z"
    }
   },
   "outputs": [],
   "source": [
    "# using UDF\n",
    "@F.udf(ArrayType(DoubleType(), True))\n",
    "def create_coords(lat, lon):\n",
    "    return round(lat, 4), round(lon, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522148b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:19.247612Z",
     "start_time": "2021-07-04T02:46:18.262554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_sdf.withColumn(\"pickup_coords\", create_coords(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276beec",
   "metadata": {},
   "source": [
    "And here's an example of mapping values from our data dictionary using a Pandas UDF:\n",
    "- Type definition Syntax: https://www.python.org/dev/peps/pep-0484/#type-definition-syntax\n",
    "- Function Decorators: https://johnpaton.net/posts/clean-spark-udfs/\n",
    "\n",
    "The Pandas UDF is also quite new so there isn't much *help* other than the documentation: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.pandas_udf.html?highlight=pandas%20udf\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "@pandas_udf(THE DATATYPE OF THE OUTPUT)\n",
    "def FUNCTION_NAME(ARGUMENTS: INPUT DATA FORMAT) -> OUTPUT DATA FORMAT:\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "sdf.withColumn(COLUMN OUT, FUNCTION_NAME(col(COLUMN IN)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec5375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:22.401172Z",
     "start_time": "2021-07-04T02:46:22.398536Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ccb16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:22.608592Z",
     "start_time": "2021-07-04T02:46:22.587296Z"
    }
   },
   "outputs": [],
   "source": [
    "vendors = {1: 'Creative Mobile Technologies, LLC', 2: 'VeriFone Inc.'}\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def vendorMap(vid_col: pd.Series) -> pd.Series:\n",
    "    return vid_col.map(vendors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360d69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T02:46:23.891004Z",
     "start_time": "2021-07-04T02:46:22.778917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_sdf.withColumn(\"VendorName\", vendorMap(col(\"VendorID\"))) \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8ffa1",
   "metadata": {},
   "source": [
    "And that's the basics of PySpark! If you would like to further increase your scope, here are some pathways:\n",
    "- Data Science: Continue with Spark's MLlib to perform machine learning.\n",
    "- Data Engineering: Learn Spark SQL and Spark Connectors (i.e connecting to data sources such as S3 buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade325a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
